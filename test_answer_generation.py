#!/usr/bin/env python3
"""
Test script for answer generation from web search using only Selenium as the source.
"""

import asyncio
import json
import logging
import os
from typing import Dict, Any, List
from datetime import datetime, timezone

from search_agent.answer_orchestrator import orchestrate_answer_generation
from search_agent.orchestrator import run_orchestration
from search_agent.config import settings

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Override the run_orchestration function to use only Selenium
original_run_orchestration = run_orchestration

async def selenium_only_orchestration(query: str) -> Any:
    """
    Modified orchestration function that only uses the Selenium module.
    """
    logger.info("Running search with Selenium module only")
    # Call the original function but specify only the selenium_search module
    return await original_run_orchestration(query, modules=["selenium_search"])

# Monkey patch the run_orchestration function in the answer_orchestrator module
import search_agent.answer_orchestrator
search_agent.answer_orchestrator.run_orchestration = selenium_only_orchestration

# Mock the LLM functions for testing
import search_agent.answer_synthesizer
import search_agent.answer_evaluator

original_synthesize_answer = search_agent.answer_synthesizer.synthesize_answer
original_evaluate_answer_quality = search_agent.answer_evaluator.evaluate_answer_quality

async def mock_synthesize_answer(query: str, content_snippets: List[str], max_retries: int = 3) -> str:
    """Mock implementation of synthesize_answer for testing."""
    logger.info("Using mock synthesize_answer function")
    
    # Create a simple answer based on the query and content
    if "today" in query.lower():
        from datetime import datetime
        today = datetime.now().strftime("%A, %B %d, %Y")
        return f"Today is {today}. This information is based on the current date according to your system clock."
    
    # Special case for clinical metabolomics
    if "clinical metabolomics" in query.lower():
        logger.info("Using mock answer for clinical metabolomics")
        return """Clinical metabolomics is the application of metabolomics in clinical settings to improve patient diagnosis, prognosis, and treatment. It involves the comprehensive study of small molecules (metabolites) in biological samples such as blood, urine, or tissues from patients.

Key aspects of clinical metabolomics include:

1. Biomarker discovery: Identifying metabolic signatures that can serve as indicators of disease states or treatment responses.

2. Personalized medicine: Tailoring medical treatments to individual patients based on their unique metabolic profiles.

3. Disease mechanism understanding: Providing insights into the biochemical pathways involved in disease development and progression.

4. Drug development and monitoring: Assessing drug efficacy, toxicity, and metabolism in patients.

Clinical metabolomics typically employs advanced analytical techniques such as mass spectrometry (MS) and nuclear magnetic resonance (NMR) spectroscopy to detect and quantify thousands of metabolites simultaneously, followed by sophisticated data analysis methods to extract meaningful clinical information."""
    
    # Default response if no specific pattern is matched
    return f"This is a mock answer to the query: '{query}'. In a real scenario, this would be generated by an LLM based on the {len(content_snippets)} content snippets extracted from search results."

async def mock_evaluate_answer_quality(query: str, synthesized_answer: str, original_content: List[str], max_retries: int = 3) -> Dict[str, Any]:
    """Mock implementation of evaluate_answer_quality for testing."""
    logger.info("Using mock evaluate_answer_quality function")
    
    # Return mock evaluation scores
    return {
        "factual_consistency_score": 0.95,
        "relevance_score": 0.90,
        "completeness_score": 0.85,
        "conciseness_score": 0.80,
        "llm_feedback": "This is a mock evaluation. The answer appears to be factually consistent with the provided content and directly addresses the query.",
        "nlp_relevance_score": 0.88
    }

# Monkey patch the LLM functions
# Comment out the following lines to use real LLM functions
# search_agent.answer_synthesizer.synthesize_answer = mock_synthesize_answer
# search_agent.answer_evaluator.evaluate_answer_quality = mock_evaluate_answer_quality
# logger.info("Using mock LLM functions for testing")
logger.info("Using real LLM functions for testing")

async def main():
    """
    Main test function.
    """
    query = "What are the main applications of metabolomics in medicine?"
    logger.info(f"Testing answer generation with query: '{query}'")
    
    try:
        # Run the answer generation pipeline
        result = await orchestrate_answer_generation(query)
        
        # Print the result in a readable format
        print("\n" + "="*80)
        print(f"QUERY: {query}")
        print("="*80)
        
        # Check if result is a dictionary or a string (error message)
        if isinstance(result, dict):
            print("\nSYNTHESIZED ANSWER:")
            print("-"*80)
            if isinstance(result.get("synthesized_answer"), dict):
                print(result.get("synthesized_answer", {}).get("answer", "No answer generated"))
            else:
                print(result.get("synthesized_answer", "No answer generated"))
        else:
            print("\nERROR:")
            print("-"*80)
            print(result)
        
        if isinstance(result, dict):
            # Print source URLs if available
            if "source_urls" in result:
                print("\nSOURCE URLS:")
                print("-"*80)
                for url in result.get("source_urls", []):
                    print(f"- {url}")
                
            # Print the extracted content if available
            if "extracted_contents" in result:
                print("\nEXTRACTED CONTENT:")
                print("-"*80)
                if not result.get("extracted_contents"):
                    print("No content was successfully extracted.")
                else:
                    for i, content in enumerate(result.get("extracted_contents", [])):
                        print(f"Content from {result.get('source_urls', [])[i] if i < len(result.get('source_urls', [])) else 'unknown'}:")
                        print(f"{content[:500]}..." if len(content) > 500 else content)
                        print("-"*40)
                
            # Print evaluation results if available
            if "evaluation_results" in result:
                print("\nEVALUATION RESULTS:")
                print("-"*80)
                eval_results = result.get("evaluation_results", {})
                print(f"Factual Consistency: {eval_results.get('factual_consistency_score', 0):.2f}")
                print(f"Relevance: {eval_results.get('relevance_score', 0):.2f}")
                print(f"Completeness: {eval_results.get('completeness_score', 0):.2f}")
                print(f"Conciseness: {eval_results.get('conciseness_score', 0):.2f}")
                print(f"LLM Feedback: {eval_results.get('llm_feedback', 'None')}")
            
            # Print metadata if available
            if "metadata" in result:
                print("\nMETADATA:")
                print("-"*80)
                metadata = result.get("metadata", {})
                print(f"Total URLs found: {metadata.get('total_urls_found', 0)}")
                print(f"URLs selected: {metadata.get('urls_selected', 0)}")
                print(f"Extraction success: {metadata.get('extraction_success_count', 0)}")
                print(f"Extraction failures: {metadata.get('extraction_failure_count', 0)}")
                print(f"Low quality content: {metadata.get('low_quality_content_count', 0)}")
                print(f"Search time: {metadata.get('search_execution_time', 0):.2f}s")
                print(f"Content extraction time: {metadata.get('content_extraction_time', 0):.2f}s")
                print(f"Answer synthesis time: {metadata.get('answer_synthesis_time', 0):.2f}s")
                print(f"Answer evaluation time: {metadata.get('answer_evaluation_time', 0):.2f}s")
                print(f"Total execution time: {result.get('execution_time_seconds', 0):.2f}s")
                
                if metadata.get("errors"):
                    print("\nERRORS:")
                    print("-"*80)
                    for error in metadata.get("errors", []):
                        print(f"- {error}")
            
            # Save the full result to a file
            # Convert non-serializable objects to strings for JSON serialization
            def convert_for_json(obj):
                if hasattr(obj, '__class__') and obj.__class__.__name__ == 'HttpUrl':
                    return str(obj)
                elif isinstance(obj, datetime):
                    return obj.isoformat()
                raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
                
            with open("answer_generation_result.json", "w") as f:
                json.dump(result, f, indent=2, default=convert_for_json)
                logger.info("Full result saved to answer_generation_result.json")
            
    except Exception as e:
        logger.error(f"Error during test: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())